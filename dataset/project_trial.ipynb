{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rdflib in /Users/kasper/datavis/venv/lib/python3.10/site-packages (7.1.1)\n",
      "Requirement already satisfied: isodate<1.0.0,>=0.7.2 in /Users/kasper/datavis/venv/lib/python3.10/site-packages (from rdflib) (0.7.2)\n",
      "Requirement already satisfied: pyparsing<4,>=2.1.0 in /Users/kasper/datavis/venv/lib/python3.10/site-packages (from rdflib) (3.1.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting owlrl\n",
      "  Downloading owlrl-7.1.2-py3-none-any.whl (51 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.7/51.7 kB\u001b[0m \u001b[31m425.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[?25hCollecting rdflib>=7.1.1\n",
      "  Using cached rdflib-7.1.1-py3-none-any.whl (562 kB)\n",
      "Collecting pyparsing<4,>=2.1.0\n",
      "  Downloading pyparsing-3.2.0-py3-none-any.whl (106 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.9/106.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting isodate<1.0.0,>=0.7.2\n",
      "  Using cached isodate-0.7.2-py3-none-any.whl (22 kB)\n",
      "Installing collected packages: pyparsing, isodate, rdflib, owlrl\n",
      "  Attempting uninstall: pyparsing\n",
      "    Found existing installation: pyparsing 3.1.1\n",
      "    Uninstalling pyparsing-3.1.1:\n",
      "      Successfully uninstalled pyparsing-3.1.1\n",
      "  Attempting uninstall: isodate\n",
      "    Found existing installation: isodate 0.7.2\n",
      "    Uninstalling isodate-0.7.2:\n",
      "      Successfully uninstalled isodate-0.7.2\n",
      "  Attempting uninstall: rdflib\n",
      "    Found existing installation: rdflib 7.1.1\n",
      "    Uninstalling rdflib-7.1.1:\n",
      "      Successfully uninstalled rdflib-7.1.1\n",
      "Successfully installed isodate-0.7.2 owlrl-7.1.2 pyparsing-3.2.0 rdflib-7.1.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pip install rdflib\n",
    "%pip install --upgrade --force-reinstall owlrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'owlrl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrdflib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Graph, Literal, Namespace, RDF, RDFS, OWL, URIRef\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrdflib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnamespace\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XSD\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mowlrl\u001b[39;00m  \u001b[38;5;66;03m# For reasoning\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'owlrl'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import rdflib\n",
    "from rdflib import Graph, Literal, Namespace, RDF, RDFS, OWL, URIRef\n",
    "from rdflib.namespace import XSD\n",
    "import owlrl  # For reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "books = pd.read_csv(\"Books.csv\")  \n",
    "ratings = pd.read_csv(\"Ratings.csv\")  \n",
    "users = pd.read_csv(\"Users.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of datasets\n",
    "\n",
    "print(f'''\\t  Size of books data is {books.shape}\n",
    "          Size of ratings data is {ratings.shape}\n",
    "          Size of users data is {users.shape}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking duplicates in datasets using duplicated method of dataframe.\n",
    "\n",
    "print(f'''\\t  Duplicates in books data is {books.duplicated().sum()}\n",
    "          Duplicates in ratings data is {ratings.duplicated().sum()}\n",
    "          Duplicates in users data is {users.duplicated().sum()}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to get the missing values count and it's percentage\n",
    "def missing_values(df):\n",
    "  \"\"\"\n",
    "  Description : This function takes a data frame as an input and gives missing value count and its percentage as an output\n",
    "  function_name : missing_values\n",
    "  Argument : dataframe.\n",
    "  Return : dataframe\n",
    "  \n",
    "  \"\"\"\n",
    "  miss = df.isnull().sum() # finding the missing values.\n",
    "  \n",
    "  per = df.isnull().mean() # finding mean/ Average of missing values.\n",
    "  df = pd.concat([miss,per*100],keys = ['Missing_Values','Percentage'], axis = 1) # concatenating both of them using concat method of pandas module.\n",
    "  return df # returning dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' STEP 1: CLEANING OF BOOKS'''\n",
    "books.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values(books)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "number of missing values for Book_author and Publisher is negegable --> drop those rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "books.drop_duplicates(inplace=True)\n",
    "# Drop rows with 'unknown' ISBN or Book_Title\n",
    "books.drop(books[books['ISBN'].str.lower() == 'unknown'].index, inplace=True)\n",
    "books.drop(books[books['Book_Title'].str.lower() == 'unknown'].index, inplace=True)\n",
    "# Clean ISBN column: remove invalid characters and spaces\n",
    "books['ISBN'] = books['ISBN'].astype(str).apply(lambda x: re.sub(r'[^a-zA-Z0-9]', '', x).strip())\n",
    "\n",
    "# Ensure the Year_Of_Publication is numeric and valid\n",
    "books['Year_Of_Publication'] = pd.to_numeric(books['Year_Of_Publication'], errors='coerce').fillna(0).astype(int)\n",
    "books.loc[(books['Year_Of_Publication'] < 1000) | (books['Year_Of_Publication']>2024), 'Year_Of_Publication'] = None  # Remove invalid years\n",
    "books['Year_Of_Publication'] = pd.to_numeric(books['Year_Of_Publication'], errors='coerce').fillna(0000).astype(int)\n",
    "# Fill null values for critical fields\n",
    "books['Publisher'] = books['Publisher'].fillna('Unknown')\n",
    "books['Book_Author'] = books['Book_Author'].fillna('Unknown')\n",
    "\n",
    "# Drop unused columns\n",
    "books = books.drop(columns=['Image_URL_S', 'Image_URL_M', 'Image_URL_L'], axis=1) # these columns are not relevant for our reccomendation system\n",
    "\n",
    "books.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books['Year_Of_Publication'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' STEP 2: CLEANING OF RATINGS'''\n",
    "ratings.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.describe(include = 'O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar to books, we will remove duplicates and clean the ISBN column\n",
    "ratings.drop_duplicates(inplace=True)\n",
    "ratings.drop(ratings[ratings['ISBN'].str.lower() == 'unknown'].index, inplace=True)\n",
    "ratings['ISBN'] = ratings['ISBN'].astype(str).apply(lambda x: re.sub(r'[^a-zA-Z0-9]', '', x).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' STEP 3: CLEANING OF USERS'''\n",
    "users.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pycountry\n",
    "# import pycountry\n",
    "# countries = [country.name.strip().lower() for country in pycountry.countries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# # Example: Your Country Column\n",
    "# users['Country'] = users['Location'].str.split(',').str[-1].str.strip().str.lower()\n",
    "\n",
    "# # Define a mapping for misspelled countries and replacements\n",
    "# country_mapping = {\n",
    "#     # Common corrections\n",
    "#     'u.s.a.': 'usa', 'u.s.a': 'usa', 'good old usa !': 'usa', 'usa\"': 'usa',\n",
    "#     'u.s. of a.': 'usa', 'america': 'usa', 'united states': 'usa',\n",
    "#     'united stated': 'usa', 'united staes': 'usa', 'unite states': 'usa',\n",
    "#     'england': 'united kingdom', 'u.k.': 'united kingdom', 'uk': 'united kingdom',\n",
    "#     'united kindgdom': 'united kingdom', 'united kindgonm': 'united kingdom',\n",
    "#     'russia': 'russian federation', 'russian federation': 'russian federation',\n",
    "#     'deutschland': 'germany', 'germay': 'germany', 'geermany': 'germany',\n",
    "#     'españa': 'spain', 'espaã±a': 'spain', 'la france': 'france',\n",
    "#     'méxico': 'mexico', 'mã?â©xico': 'mexico',\n",
    "#     'italia': 'italy', 'itlay': 'italy', 'italy\"': 'italy',\n",
    "#     'catalunya spain': 'spain', 'brasil': 'brazil', 'brazil\"': 'brazil',\n",
    "#     'suisse': 'switzerland', 'la suisse': 'switzerland', 'switzerland\"': 'switzerland',\n",
    "#     'netherlands\"': 'netherlands', 'holland': 'netherlands',\n",
    "#     'u.a.e': 'united arab emirates', 'u.a.e\"': 'united arab emirates',\n",
    "#     'uae': 'united arab emirates',\n",
    "#     'phillipines': 'philippines', 'phippines': 'philippines',\n",
    "#     'india\"': 'india',\n",
    "#     # Handle special regions and nonsense\n",
    "#     'somewherein space': 'unknown', 'space': 'unknown', 'n/a': 'unknown',\n",
    "#     'n/a - on the road': 'unknown', 'nowhere': 'unknown',\n",
    "#     'in your heart': 'unknown', 'home of the van!!': 'unknown',\n",
    "#     'everywhere and anywhere': 'unknown', 'strongbadia': 'unknown',\n",
    "#     'mordor': 'unknown', 'evil empire': 'unknown', 'fairyland': 'unknown',\n",
    "#     'unknown': 'unknown', 'aaa': 'unknown', '-': 'unknown', '.': 'unknown', \n",
    "#     '...': 'unknown', '????': 'unknown', '*': 'unknown'\n",
    "# }\n",
    "\n",
    "# # Function to clean country names\n",
    "# def clean_country(country):\n",
    "#     # Check if the country is in the mapping dictionary\n",
    "#     if country in country_mapping:\n",
    "#         return country_mapping[country]\n",
    "#     # Remove numeric and special characters except letters, spaces, and hyphens\n",
    "#     cleaned = re.sub(r'[^a-zA-Z\\s\\-]', '', country).strip()\n",
    "#     # If still invalid, return 'unknown'\n",
    "#     return cleaned if cleaned else 'unknown'\n",
    "\n",
    "# # Apply the cleaning function\n",
    "# users['Country'] = users['Country'].apply(clean_country)\n",
    "\n",
    "# # Check unique cleaned countries\n",
    "# print(users['Country'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "users['Age'] = pd.to_numeric(users['Age'], errors='coerce').fillna(-1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define namespaces\n",
    "EX = Namespace(\"http://example.org/bookRec#\")\n",
    "SCHEMA = Namespace(\"http://schema.org/\")\n",
    "FOAF = Namespace(\"http://xmlns.com/foaf/0.1/\")\n",
    "\n",
    "# Create RDF graph\n",
    "g = Graph()\n",
    "g.bind(\"ex\", EX)\n",
    "g.bind(\"schema\", SCHEMA)\n",
    "g.bind(\"foaf\", FOAF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add books to RDF graph\n",
    "for _, row in books.iterrows():\n",
    "    book_uri = URIRef(EX[f\"Book_{row['ISBN']}\"]) # unique identifier for each book\n",
    "    g.add((book_uri, RDF.type, EX.Book))\n",
    "    g.add((book_uri, RDFS.label, Literal(row['Book_Title'], lang=\"en\")))\n",
    "    g.add((book_uri, EX.author, Literal(row['Book_Author'])))\n",
    "    g.add((book_uri, EX.publisher, Literal(row['Publisher'])))\n",
    "    g.add((book_uri, EX.year, Literal(row['Year_Of_Publication'], datatype=XSD.gYear)))\n",
    "\n",
    "# Serialize the graph\n",
    "print(\"Serialized RDF Graph:\")\n",
    "print(g.serialize(format=\"turtle\", indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in users.iterrows():\n",
    "    user_uri = URIRef(EX[f\"User_{row['User-ID']}\"])\n",
    "    g.add((user_uri, RDF.type, FOAF.Person))\n",
    "    g.add((user_uri, FOAF.id, Literal(f\"User_{row['User-ID']}\")))\n",
    "    g.add((user_uri, EX.location, Literal(row['Location'])))\n",
    "    if row['Age'] != -1:\n",
    "        g.add((user_uri, FOAF.age, Literal(row['Age'], datatype=XSD.integer)))\n",
    "\n",
    "# Serialize the graph\n",
    "print(\"Serialized RDF Graph:\")\n",
    "print(g.serialize(format=\"turtle\", indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in ratings.iterrows():\n",
    "    user_uri = URIRef(EX[f\"User_{row['User-ID']}\"])\n",
    "    book_uri = URIRef(EX[f\"Book_{row['ISBN']}\"])\n",
    "    g.add((user_uri, EX.rated, book_uri))\n",
    "    g.add((user_uri, EX.rating, Literal(row['Book-Rating'], datatype=XSD.integer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize the RDF graph to Turtle format\n",
    "ttl_file = 'bookData.ttl'\n",
    "g.serialize(destination=ttl_file, format='turtle')\n",
    "\n",
    "print(f\"RDF data has been successfully converted and stored in {ttl_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
